{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 10 Emoji Prediction\n",
    "\n",
    "The task of this project is to make a system that would automatically fill the text with the appropriate emoticons. This can be done in two steps. First, for each position within the text a prediction is made whether an emoticon should be placed there. Second, an appropriate emoticon is chosen from a list of available emoticons. Both these tasks can be set up as supervised classification problems.\n",
    "\n",
    "Competition website:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Dataset:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Entry point:\n",
    "https://arxiv.org/pdf/1702.07285.pdf (Barbieri, Francesco, Miguel Ballesteros, and Horacio Saggion. Are Emojis Predictable?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define paths to folders containing the data and the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_TWEETS = None\n",
    "# NUMBER_OF_TWEETS = 100000\n",
    "\n",
    "MAX_WORDS_PER_TWEET = 30\n",
    "# DATA_LOCATION = \"./train/data/noise/\"\n",
    "DATA_LOCATION = \"./train/data/\"\n",
    "# DATA_LOCATION = \"./train/data/words/\"\n",
    "RESULT_LOCATION = \"./results/\"\n",
    "#TWEET_FILE_NAME = \"tweet_by_ID_28_4_2018__03_20_05\" + \"_\"\n",
    "TWEET_FILE_NAME = \"tweet_by_ID_02_6_2018__05_40_21__no_hashtags\" + \"_\"\n",
    "\n",
    "k = 3\n",
    "\n",
    "if NUMBER_OF_TWEETS is not None:\n",
    "    TWEET_FILE_NAME += str(NUMBER_OF_TWEETS)\n",
    "else:\n",
    "    TWEET_FILE_NAME += \"ALL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the data\n",
    "Tweets are loaded in two ways: list of strings (for the TF-IDF vectorizer) and a list of lists of words (for feature extraction). Labels are read as a numpy array of N * MAX_WORDS_PER_TWEET dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets 473459\n",
      "example of tweet texts:\n",
      "'lol @ west covina , california'\n",
      "'things got a little festive at the office @ redrock'\n",
      "'step out and explore . @ ellis island cafe'\n",
      "'@user @ cathedral preparatory school'\n",
      "'my baby bear @ bubby's'\n",
      "'rupaul's drag race bingo fun . drag queens be sexy ! @user abwyman'\n",
      "'black history like a mufffffaaaaaka done thru her yugioh trap card like hell'\n",
      "'just light makeup'\n",
      "'@ bj's restaurant and brewhouse'\n",
      "'so lovely catching up with my soul sister @user @ university of victoria'\n",
      "\n",
      "example of labels (emoji locations):\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "example of labels (emoji type):\n",
      "[[ 0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "base_file_name = DATA_LOCATION + TWEET_FILE_NAME\n",
    "\n",
    "text_lines = []\n",
    "text_lines_split = []\n",
    "\n",
    "with open(base_file_name + \".text\", 'r', encoding=\"utf-8\") as out_text:\n",
    "    for line in out_text:\n",
    "        text_lines.append(line[:-1])\n",
    "        text_lines_split.append(line[:-1].split())\n",
    "        \n",
    "loc_lines = []\n",
    "with open(base_file_name + \".loclabels\", 'r') as loc_labels:\n",
    "    for line in loc_labels:\n",
    "        loc_line = []\n",
    "        for c in line[:-1]:\n",
    "            loc_line.append(int(c))\n",
    "        loc_lines.append(loc_line)\n",
    "\n",
    "loc_lines = np.asarray(loc_lines)\n",
    "\n",
    "emo_lines = []\n",
    "with open(base_file_name + \".emolabels\", 'r') as emo_labels:\n",
    "    for e_line, loc in zip(emo_labels, loc_lines):\n",
    "        emo_line = [0]*31\n",
    "        e_line2 = e_line.split()\n",
    "        \n",
    "        br = 0\n",
    "        for idx, val in enumerate(loc[:-1]):\n",
    "            if val==1:\n",
    "                emo_line[idx]=int(e_line2[br])+1\n",
    "                br += 1\n",
    "        emo_lines.append(emo_line)\n",
    "        \n",
    "emo_lines = np.asarray(emo_lines)\n",
    "\n",
    "print(f\"number of tweets {len(text_lines)}\")\n",
    "print(f\"example of tweet texts:\")\n",
    "for i in range(10):\n",
    "    print(f\"'{text_lines[i]}'\")\n",
    "print(f\"\\nexample of labels (emoji locations):\\n{loc_lines[:10]}\")\n",
    "print(f\"\\nexample of labels (emoji type):\\n{emo_lines[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### TF-IDF Feature Extraction\n",
    "TF-IDF is computed on the collection of tweets. Then for every position between words a new example is generated: a 2 * k array containing the k left and k right tfidf values of words. \n",
    "Labels for locations are taken as 1 or 0 wether an emoji was there in the original tweet.\n",
    "Labels for emojis are mapped to a number between 0 and 20 where 20 marks no appearance of emoji and the rest describes 20 most frequent emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "\n",
    "X_tfidf = None\n",
    "word_to_tfidf_index_dict = {}\n",
    "\n",
    "def tfidf_features(tweets, loc_labels, emo_labels, k, func):\n",
    "    global word_to_tfidf_index_dict, X_tfidf\n",
    "    \n",
    "    tfidf_model = TfidfVectorizer(input=\"content\", analyzer=\"word\", stop_words=\"english\")\n",
    "    X_tfidf = tfidf_model.fit_transform(text_lines)\n",
    "\n",
    "    #word_to_tfidf_index_dict = {}\n",
    "    for i, word in enumerate(tfidf_model.get_feature_names()):\n",
    "        word_to_tfidf_index_dict[word] = i\n",
    "\n",
    "    print(f\"X shape {X_tfidf.shape}\")\n",
    "    print(f\"y_l shape {loc_lines.shape}\")\n",
    "    print(f\"y_e shape {emo_lines.shape}\")\n",
    "    print(f\"some tf-idf values\\n{X_tfidf[0]}\\n\")\n",
    "\n",
    "    N = len(tweets)\n",
    "    X = []\n",
    "    y_l = []\n",
    "    y_e = []\n",
    "    \n",
    "    for tweet_index, (tweet, l_label, e_label) in enumerate(zip(tweets, loc_labels, emo_labels)):\n",
    "        for pos in range(len(tweet) + 1):\n",
    "                \n",
    "            x = []\n",
    "            for i in range(pos - k, pos + k):\n",
    "                if i < 0 or i >= len(tweet):\n",
    "                    x.append(0.0)\n",
    "                else:\n",
    "                    x.append(func(tweet_index, tweet[i]))\n",
    "            X.append(x)\n",
    "            y_l.append(l_label[pos])\n",
    "            y_e.append(e_label[pos])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y_l), np.asarray(y_e)\n",
    "\n",
    "def word_to_tfidf(tweet_index, word):\n",
    "    global word_to_tfidf_index_dict, X_tfidf\n",
    "    \n",
    "    if word in word_to_tfidf_index_dict:\n",
    "        return X_tfidf[tweet_index, word_to_tfidf_index_dict[word]]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def print_dataset(X, y_l, y_e):\n",
    "    emoji_num = np.count_nonzero(y_l)\n",
    "    class_freq_ratio = emoji_num / (X.shape[0] * X.shape[1])\n",
    "    \n",
    "    print(\"after feature extraction:\")\n",
    "    print(f\"X shape {X.shape}\")\n",
    "    print(f\"y_l shape {y_l.shape}\")\n",
    "    print(f\"y_e shape {y_e.shape}\")\n",
    "    print(f\"some X values\\n{X[:5]}\")\n",
    "    print(f\"some y_l values\\n{y_l[:5]}\")\n",
    "    print(f\"non zero elements (1 in label) in y_l {emoji_num}\")\n",
    "    print(f\"class frequency ratio {class_freq_ratio}\\n\")\n",
    "    print(f\"some y_e values\\n{y_e[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vector Feature Extraction\n",
    "\n",
    "A similar dataset is obtained using w2vec. By setting the number of features to $2*k=6$, the dimensionality stays the same as the previous tf-idf feature extraction. For each word a 6 dimensional vector is obtained and then the average of the 6 neighboring words is taken. Labels stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "num_of_features = 2 * k\n",
    "w2v_model_file_name = \"./models/w2v_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(text_lines_split, min_count=1, size=num_of_features)\n",
    "w2v_model.save(w2v_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "[ 8.471104    4.5863085   0.05026486 -2.7232955  -4.0103526   6.726843  ]\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(w2v_model_file_name)\n",
    "\n",
    "example = w2v_model.wv[\"california\"]\n",
    "print(example.shape)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_features(tweets, labels, k):\n",
    "    N = len(tweets)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for tweet_index, (tweet, label) in enumerate(zip(tweets, labels)):\n",
    "        for pos in range(len(tweet) + 1):\n",
    "            x = []\n",
    "            for i in range(pos - k, pos + k):\n",
    "                if i < 0 or i >= len(tweet):\n",
    "                    x.append(np.zeros(num_of_features))\n",
    "                else:\n",
    "                    x.append(w2v_model.wv[tweet[i]])\n",
    "            x = np.average(x, axis=0)\n",
    "            X.append(x)\n",
    "            y.append(label[pos])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (473459, 134815)\n",
      "y_l shape (473459, 31)\n",
      "y_e shape (473459, 31)\n",
      "some tf-idf values\n",
      "  (0, 73855)\t0.41212570578147684\n",
      "  (0, 129964)\t0.4318968334528044\n",
      "  (0, 31497)\t0.7178895542721762\n",
      "  (0, 23363)\t0.35812304555062646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# should_use_tfidf = True\n",
    "# should_use_hybrid = False\n",
    "\n",
    "# X_tfidf, y_tfidf = tfidf_features(text_lines_split, loc_lines, k, word_to_tfidf)\n",
    "# X_w2v, y_w2v = w2v_features(text_lines_split, loc_lines, k)\n",
    "# X_hybrid = np.hstack((X_tfidf, X_w2v))\n",
    "\n",
    "# if should_use_tfidf or should_use_hybrid:    \n",
    "#     X, y = X_tfidf, y_tfidf\n",
    "\n",
    "# if (not should_use_tfidf) or should_use_hybrid:\n",
    "#     X, y = X_w2v, y_w2v\n",
    "\n",
    "# if should_use_hybrid:\n",
    "#     X = X_hybrid\n",
    "\n",
    "# print_dataset(X, y)\n",
    "\n",
    "X_tfidf, y_loc_tfidf, y_emo_tfidf = tfidf_features(text_lines_split, loc_lines, emo_lines, k, word_to_tfidf)\n",
    "\n",
    "X_train, X_test, y_loc_train, y_loc_test = train_test_split(X_tfidf, y_loc_tfidf, shuffle=False, test_size=0.1, random_state=42)\n",
    "X_train, X_test, y_emo_train, y_emo_test = train_test_split(X_tfidf, y_emo_tfidf, shuffle=False, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Baselines\n",
    "\n",
    "Make sure to check should_train flags when training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "should_use_kfold = True\n",
    "\n",
    "should_train_global = True\n",
    "should_train_linear_svm = True\n",
    "should_train_bagging_svm = False\n",
    "should_train_random_forest = False\n",
    "should_train_adaboost = True\n",
    "\n",
    "baseline_models_dir = \"./models/baseline_models/\"\n",
    "linear_svm_model_file_name = baseline_models_dir + \"linear_svm.pkl\"\n",
    "bagging_model_file_name = baseline_models_dir + \"bagging_svm.pkl\"\n",
    "random_forest_file_name = baseline_models_dir + \"random_forest.pkl\"\n",
    "adaboost_file_name = baseline_models_dir + \"adaboost.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "EVALUATION_RESULTS_KEYS = ['Accuracy: ', 'Precision: ', 'Recall: ', 'F1: ', 'ratio of positive/negative predictions: ']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "def calc_scores(y_pred, y_test, multi_class):\n",
    "    result = []\n",
    "    \n",
    "    if multi_class:\n",
    "        result_temp = []\n",
    "    \n",
    "        for i in range(0, 21):\n",
    "            vfunc = np.vectorize(vectorize_func)\n",
    "            y_test_new = vfunc(y_test, i)\n",
    "            y_pred_new = vfunc(y_pred, i)\n",
    "            \n",
    "            result_temp2 = []\n",
    "            alfa=1\n",
    "            if i == 0:\n",
    "                alfa=1/5\n",
    "            \n",
    "            result_temp2.append(alfa*accuracy_score(y_pred_new, y_test_new))\n",
    "            result_temp2.append(alfa*precision_score(y_pred_new, y_test_new))\n",
    "            result_temp2.append(alfa*recall_score(y_pred_new, y_test_new))\n",
    "            result_temp2.append(alfa*f1_score(y_pred_new, y_test_new))\n",
    "            result_temp.append(result_temp2)\n",
    "    \n",
    "        result_avg = np.average(np.asarray(result_temp), axis=0)\n",
    "        result = result_avg[:]\n",
    "    else:\n",
    "        result.append(accuracy_score(y_pred, y_test))\n",
    "        result.append(precision_score(y_pred, y_test))\n",
    "        result.append(recall_score(y_pred, y_test))\n",
    "        result.append(f1_score(y_pred, y_test))\n",
    "        result.append(np.count_nonzero(y_pred) / y_pred.shape[0])\n",
    "    return result\n",
    "\n",
    "def print_scores(scores):\n",
    "    print(f\"Accuracy: {scores[0]}\")\n",
    "    print(f\"Precision: {scores[1]}\")\n",
    "    print(f\"Recall: {scores[2]}\")\n",
    "    print(f\"F1: {scores[3]}\")\n",
    "    if len(scores) > 4:\n",
    "        print(f\"ratio of positive/negative predictions {scores[4]}\")\n",
    "\n",
    "def kfold_score(clf, X, y, multi_class, should_print=True):\n",
    "    fold_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_kf, X_test_kf = X[train_index], X[test_index]\n",
    "        y_train_kf, y_test_kf = y[train_index], y[test_index]\n",
    "        clf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred = clf.predict(X_test_kf)\n",
    "        fold_scores.append(calc_scores(y_pred, y_test_kf, multi_class))\n",
    "        \n",
    "    fold_scores = np.average(np.asarray(fold_scores), axis=0)\n",
    "    if should_print:\n",
    "        print_scores(fold_scores)\n",
    "    return fold_scores\n",
    "\n",
    "def train_and_save_model(model, X_train, y_train, model_file_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    joblib.dump(model, model_file_name)\n",
    "    \n",
    "def load_and_test_model(model_file_name, X_test, y_test):\n",
    "    clf_loaded = joblib.load(model_file_name)\n",
    "    y_pred = clf_loaded.predict(X_test)\n",
    "    print_scores(calc_scores(y_pred, y_test))\n",
    "    \n",
    "def do_test(clf, X, y, clf_file_name, flag, multi_class=False):\n",
    "    if should_use_kfold:\n",
    "        return kfold_score(clf, X, y, multi_class)\n",
    "    else:\n",
    "        if should_train_global and flag:\n",
    "            train_and_save_model(clf, X_train, y_train, clf_file_name)\n",
    "        return load_and_test_model(clf_file_name, X_test, y_test, multi_class) \n",
    "        \n",
    "def write_to_file(best_params, results, baseline_name, loc_or_emo):\n",
    "    timestr = time.strftime(\"%d_%m_%Y__%H_%M_%S_\")\n",
    "    FOLDER_LOCATION = RESULT_LOCATION + \"baselines/\" + baseline_name + \"/\"\n",
    "    FILE_NAME = baseline_name + \"_\" + loc_or_emo + \"_\" + timestr\n",
    "    if NUMBER_OF_TWEETS is not None:\n",
    "        FILE_NAME += str(NUMBER_OF_TWEETS)\n",
    "    else:\n",
    "        FILE_NAME += \"ALL\"\n",
    "    FILE_NAME += '.text'\n",
    "    RESULTS_FILE_NAME = FOLDER_LOCATION + FILE_NAME\n",
    "\n",
    "    file = open(RESULTS_FILE_NAME, \"w+\")\n",
    "    file.write(baseline_name + \", \" + loc_or_emo + \":\\n\\n\")\n",
    "    for key, val in best_params.items():\n",
    "        file.write(key+ \": \"+str(val)+\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    for i, res in enumerate(results):\n",
    "        file.write(EVALUATION_RESULTS_KEYS[i]+str(res)+\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "def emo_vectorize_func(a, b):\n",
    "    if b == 1:\n",
    "        return a\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def vectorize_func(a, b):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Emoji location prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95331697 0.01254376 0.0125267  0.0125352 ]\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(X_train, y_loc_train)\n",
    "dummy_loc_pred = dummy_clf.predict(X_test)\n",
    "print(calc_scores(y_loc_test, dummy_loc_pred, multi_class=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Emoji type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95275681 0.01218849 0.01216014 0.01217381]\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(X_train, y_emo_train)\n",
    "dummy_emo_pred = dummy_clf.predict(X_test)\n",
    "print(calc_scores(y_emo_test, dummy_emo_pred, multi_class=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Linear SVM Baseline\n",
    "\n",
    "Linear SVM is based on the liblinear library and is faster on large datasets. Not using dual optimization problem makes the training extremely fast (pseudoinverse). It is the fastest so both feature extraction methods (TF-IDF and word2vec) are used for comparison. TF-IDF works better, but adding word2vec to it (hybrid approach) adds a little bit of improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Emoji location prediction\n",
    "\n",
    "First, hyperparameter C is fine-tuned and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23028489 0.23028376 0.23029767 0.23028452 0.23030461 0.23029545\n",
      " 0.23028143 0.23029254 0.23028462 0.2302894  0.23030714 0.23028481\n",
      " 0.23028517 0.23028792 0.23029287 0.23028203 0.23029724 0.23029529\n",
      " 0.23029702 0.2302811  0.23026859]\n",
      "C: 32\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.58      0.72    540504\n",
      "          1       0.14      0.74      0.23     48199\n",
      "\n",
      "avg / total       0.89      0.59      0.68    588703\n",
      "\n",
      "Emoji locations:\n",
      "\n",
      "TF-IDF results:\n",
      "Accuracy: 0.5936804401722894\n",
      "Precision: 0.7419172790666412\n",
      "Recall: 0.1362488768836736\n",
      "F1: 0.23021844010072906\n",
      "ratio of positive/negative predictions 0.44594263595817674\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning\n",
    "\n",
    "c_range = (-5, 15)\n",
    "param_C = [2**i for i in range(c_range[0], c_range[1]+1)]\n",
    "\n",
    "c_size = c_range[1]-c_range[0]+1\n",
    "results_test = np.zeros(c_size, dtype = np.float64)\n",
    "for i in range(c_size):\n",
    "    C1 = param_C[i]\n",
    "            \n",
    "    svm_clf = LinearSVC(class_weight=\"balanced\", dual=False, C=C1)\n",
    "    results = kfold_score(svm_clf, X_train, y_loc_train, multi_class=False, should_print=False)\n",
    "    results_test[i] = results[3]\n",
    "    \n",
    "print(results_test)\n",
    "Ci = results_test.argmax()\n",
    "C_best = param_C[Ci]\n",
    "\n",
    "best_params = {}\n",
    "best_params['C'] = C_best\n",
    "\n",
    "best_estimator = LinearSVC(class_weight=\"balanced\", dual=False, C=C_best)\n",
    "best_estimator.fit(X_train, y_loc_train)\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_loc_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji locations:\\n\")\n",
    "print(\"TF-IDF results:\")\n",
    "results = do_test(best_estimator, X_tfidf, y_loc_tfidf, linear_svm_model_file_name, should_train_linear_svm)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"linearSVM\", \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5943047682787416\n",
      "Precision: 0.7397248905578954\n",
      "Recall: 0.13611358196248802\n",
      "F1: 0.2299204880345132\n",
      "ratio of positive/negative predictions 0.44494932079503585\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the best estimator\n",
    "\n",
    "svm_clf = LinearSVC(class_weight=\"balanced\", dual=False, C=0.05)\n",
    "svm_clf.fit(X_train, y_loc_train)\n",
    "y_loc_pred_svm = svm_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_loc_pred_svm, y_loc_test, False)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Emoji type prediction\n",
    "\n",
    "First, hyperparameter C is fine-tuned and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01786508 0.01786551 0.01786535 0.01786492 0.01786473 0.01786443\n",
      " 0.01786448 0.01786441 0.01786515 0.0178658  0.01786571 0.01786582\n",
      " 0.01786452 0.01786481 0.01786438 0.01786558 0.01786561 0.01786529\n",
      " 0.0178654  0.01786538 0.01786439]\n",
      "C: 64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.96    540510\n",
      "          1       0.00      0.00      0.00     10543\n",
      "          2       0.00      0.00      0.00      4967\n",
      "          3       0.00      0.00      0.00      5000\n",
      "          4       0.00      0.00      0.00      2599\n",
      "          5       0.00      0.00      0.00      2468\n",
      "          6       0.00      0.00      0.00      2207\n",
      "          7       0.00      0.00      0.00      1985\n",
      "          8       0.00      0.00      0.00      1966\n",
      "          9       0.00      0.00      0.00      1706\n",
      "         10       0.00      0.00      0.00      1532\n",
      "         11       0.00      0.00      0.00      1544\n",
      "         12       0.01      0.00      0.01      1459\n",
      "         13       0.02      0.25      0.04      1398\n",
      "         14       0.00      0.00      0.00      1223\n",
      "         15       0.00      0.00      0.00      1306\n",
      "         16       0.00      0.00      0.00      1318\n",
      "         17       0.00      0.00      0.00      1294\n",
      "         18       0.01      0.00      0.00      1243\n",
      "         19       0.00      0.00      0.00      1284\n",
      "         20       0.00      0.00      0.00      1151\n",
      "\n",
      "avg / total       0.85      0.90      0.88    588703\n",
      "\n",
      "Emoji types:\n",
      "\n",
      "TF-IDF results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9560166773259191\n",
      "Precision: 0.020951892081461358\n",
      "Recall: 0.013606945838380565\n",
      "F1: 0.011532319107019644\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning\n",
    "\n",
    "c_range = (-5, 15)\n",
    "param_C = [2**i for i in range(c_range[0], c_range[1]+1)]\n",
    "\n",
    "c_size = c_range[1]-c_range[0]+1\n",
    "results_test = np.zeros(c_size, dtype = np.float64)\n",
    "for i in range(c_size):\n",
    "    C1 = param_C[i]\n",
    "            \n",
    "    svm_clf = LinearSVC(class_weight=\"balanced\", dual=False, C=C1)\n",
    "    results = kfold_score(svm_clf, X_train, y_loc_train, multi_class=True, should_print=False)\n",
    "    results_test[i] = results[3]\n",
    "\n",
    "print(results_test)\n",
    "Ci = results_test.argmax()\n",
    "C_best = param_C[Ci]\n",
    "\n",
    "best_params = {}\n",
    "best_params['C'] = C_best\n",
    "\n",
    "best_estimator = LinearSVC(class_weight=\"balanced\", dual=False, C=C_best)\n",
    "best_estimator.fit(X_train, y_emo_train)\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_emo_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji types:\\n\")\n",
    "print(\"TF-IDF results:\")\n",
    "results = do_test(best_estimator, X_tfidf, y_emo_tfidf, linear_svm_model_file_name, should_train_linear_svm, True)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"linearSVM\", \"emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/marin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9560279202958106\n",
      "Precision: 0.021565301722035597\n",
      "Recall: 0.010961516946732256\n",
      "F1: 0.011549760583579058\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the best estimator\n",
    "\n",
    "svm_clf = LinearSVC(class_weight=\"balanced\", dual=False, C=64)\n",
    "svm_clf.fit(X_train, y_emo_train)\n",
    "y_emo_pred_svm = svm_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_emo_pred_svm, y_emo_test, True)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Merging emoji type prediction and emoji location prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(emo_vectorize_func)\n",
    "y_emo_loc_pred_svm = vfunc(y_emo_pred_svm, y_loc_pred_svm)\n",
    "\n",
    "print(f\"y_pred for emojis:\\n {y_emo_pred_svm[:400]}\\n\")\n",
    "print(f\"y_pred for locations:\\n {y_loc_pred_svm[:400]}\\n\")\n",
    "print(f\"y_pred for emojis with locations:\\n {y_emo_loc_pred_svm[:400]}\\n\")\n",
    "\n",
    "fold_scores = calc_scores(y_emo_loc_pred_svm, y_emo_test, True)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Bagging SVM\n",
    "\n",
    "Warning! SVM is based on the libsvm library and it scales poorly with large datasets. That is why an ensemble (bagging) is used. Each classifier is trained on a portion of the data which greatly reduces training times and gives similar (if not better) results. Using 10 estimators it still takes a few hours to train. Results are just a bit better than a single linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "n_estimators = 10\n",
    "bagging_svm_clf = BaggingClassifier(SVC(kernel='linear', class_weight='balanced'), max_samples=1.0 / n_estimators,\n",
    "                                        n_estimators=n_estimators, bootstrap=False)\n",
    "\n",
    "\n",
    "print(\"Emoji locations:\\n\")\n",
    "\n",
    "do_test(bagging_svm_clf, X, y_l, bagging_model_file_name, should_train_bagging_svm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "n_estimators = 10\n",
    "bagging_svm_clf = BaggingClassifier(SVC(kernel='linear', class_weight='balanced'), max_samples=1.0 / n_estimators,\n",
    "                                        n_estimators=n_estimators, bootstrap=False)\n",
    "\n",
    "print(\"Emoji types:\\n\")\n",
    "\n",
    "do_test(bagging_svm_clf, X, y_e, bagging_model_file_name, should_train_bagging_svm, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Random Forest Baseline\n",
    "\n",
    "Random forests are pretty fast and are generally better than the SVM. Higher accuracy and recall, but worse precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Emoji location prediction\n",
    "\n",
    "First, hyperparameters min_samples_leaf and min_samples_split are fine-tuned using grid search and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "scoring= 'f1'\n",
    "\n",
    "# run grid search\n",
    "clf = GridSearchCV(random_forest_clf, param_grid, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_loc_train)\n",
    "\n",
    "# results\n",
    "best_estimator = clf.best_estimator_\n",
    "best_params = {}\n",
    "best_params['min_samples_leaf'] = clf.best_params_['min_samples_leaf']\n",
    "best_params['min_samples_split'] = clf.best_params_['min_samples_split']\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_loc_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji locations:\\n\")\n",
    "results = do_test(best_estimator, X_tfidf, y_loc_tfidf, random_forest_file_name, should_train_random_forest)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"randomForest\", \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Training and testing the best estimator\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(min_samples_leaf=1, min_samples_split=5)\n",
    "random_forest_clf.fit(X_train, y_loc_train)\n",
    "\n",
    "y_loc_pred_rf = random_forest_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_loc_pred_rf, y_loc_test, False)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Emoji type prediction\n",
    "\n",
    "First, hyperparameters min_samples_leaf and min_samples_split are fine-tuned using grid search and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "scoring= 'f1_micro'\n",
    "\n",
    "# run grid search\n",
    "clf = GridSearchCV(random_forest_clf, param_grid, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_emo_train)\n",
    "\n",
    "# results\n",
    "best_estimator = clf.best_estimator_\n",
    "best_params = {}\n",
    "best_params['min_samples_leaf'] = clf.best_params_['min_samples_leaf']\n",
    "best_params['min_samples_split'] = clf.best_params_['min_samples_split']\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_emo_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji types:\\n\")\n",
    "results = do_test(best_estimator, X_tfidf, y_emo_tfidf, random_forest_file_name, should_train_random_forest, True)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"randomForest\", \"emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and testing the best estimator\n",
    "random_forest_clf = RandomForestClassifier(min_samples_leaf=4, min_samples_split=10)\n",
    "random_forest_clf.fit(X_train, y_emo_train)\n",
    "\n",
    "y_emo_pred_rf = random_forest_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_emo_pred_rf, y_emo_test, True)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Merging emoji type prediction and emoji location prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(emo_vectorize_func)\n",
    "y_emo_loc_pred_rf = vfunc(y_emo_pred_rf, y_loc_pred_rf)\n",
    "\n",
    "print(f\"y_pred for emojis:\\n {y_emo_pred_rf[:400]}\\n\")\n",
    "print(f\"y_pred for locations:\\n {y_loc_pred_rf[:400]}\\n\")\n",
    "print(f\"y_pred for emojis with locations:\\n {y_emo_loc_pred_rf[:400]}\\n\")\n",
    "\n",
    "fold_scores = calc_scores(y_emo_loc_pred_rf, y_emo_test, True)\n",
    "print_scores(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### AdaBoost Baseline\n",
    "\n",
    "AdaBoost gives the best results and is relatively fast to train. It is slower than random forests, but still faster than bagging svms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Emoji location prediction\n",
    "\n",
    "First, hyperparameters base_estimator_max_depth, base_estimator_max_features and n_estimators are fine-tuned using grid search and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree_clf = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dtree_clf)\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "param_grid = {'base_estimator__max_depth': [2, 3, 4],\n",
    "              'base_estimator__max_features': [2, 3, 4],\n",
    "              'n_estimators': [50, 75]}\n",
    "scoring = 'f1'\n",
    "\n",
    "# run grid search\n",
    "clf = GridSearchCV(adaboost_clf, param_grid, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_loc_train)\n",
    "\n",
    "# results\n",
    "best_estimator = clf.best_estimator_\n",
    "\n",
    "best_params = {}\n",
    "best_params['base_estimator__max_depth'] = clf.best_params_['base_estimator__max_depth']\n",
    "best_params['base_estimator__max_features'] = clf.best_params_['base_estimator__max_features']\n",
    "best_params['n_estimators'] = clf.best_params_['n_estimators']\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_loc_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji locations:\\n\")\n",
    "results = do_test(best_estimator, X_tfidf, y_loc_tfidf, adaboost_file_name, should_train_adaboost)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"adaBoost\", \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Training and testing the best estimator\n",
    "\n",
    "dtree_clf = DecisionTreeClassifier(max_depth = 4, max_features = 4, random_state = 11, class_weight = \"balanced\")\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dtree_clf, n_estimators = 75)\n",
    "\n",
    "adaboost_clf.fit(X_train, y_loc_train)\n",
    "\n",
    "y_loc_pred_ab = adaboost_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_loc_pred_ab, y_loc_test, False)\n",
    "print_scores(fold_scores, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Emoji type prediction\n",
    "\n",
    "First, hyperparameters base_estimator_max_depth, base_estimator_max_features and n_estimators are fine-tuned using grid search and then results for the best estimator are calculated and written into a file. \n",
    "<br>\n",
    "Second, the best estimator is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree_clf = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dtree_clf)\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "param_grid = {'base_estimator__max_depth': [2, 3, 4],\n",
    "              'base_estimator__max_features': [2, 3, 4],\n",
    "              'n_estimators': [50, 75]}\n",
    "scoring = 'f1_micro'\n",
    "\n",
    "# run grid search\n",
    "clf = GridSearchCV(adaboost_clf, param_grid, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_emo_train)\n",
    "\n",
    "# results\n",
    "best_estimator = clf.best_estimator_\n",
    "\n",
    "best_params = {}\n",
    "best_params['base_estimator__max_depth'] = clf.best_params_['base_estimator__max_depth']\n",
    "best_params['base_estimator__max_features'] = clf.best_params_['base_estimator__max_features']\n",
    "best_params['n_estimators'] = clf.best_params_['n_estimators']\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "for key, val in best_params.items():\n",
    "    print(key+ \": \"+str(val))\n",
    "print(classification_report(y_emo_test, y_pred))\n",
    "\n",
    "\n",
    "# K-fold \n",
    "\n",
    "print(\"Emoji locations:\\n\")\n",
    "results = do_test(best_estimator, X_tfidf, y_emo_tfidf, adaboost_file_name, should_train_adaboost, True)\n",
    "\n",
    "# Writing results into a file\n",
    "write_to_file(best_params, results, \"adaBoost\", \"emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and testing the best estimator\n",
    "\n",
    "dtree_clf = DecisionTreeClassifier(max_depth = 2, max_features = 2, random_state = 11, class_weight = \"balanced\")\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dtree_clf, n_estimators = 50)\n",
    "\n",
    "adaboost_clf.fit(X_train, y_emo_train)\n",
    "\n",
    "y_emo_pred_ab = adaboost_clf.predict(X_test)\n",
    "fold_scores = calc_scores(y_emo_pred_ab, y_emo_test, True)\n",
    "print_scores(fold_scores, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Merging emoji type prediction and emoji location prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(emo_vectorize_func)\n",
    "y_emo_loc_pred_ab = vfunc(y_emo_pred_ab, y_loc_pred_ab)\n",
    "\n",
    "print(f\"y_pred for emojis:\\n {y_emo_pred_ab[:400]}\\n\")\n",
    "print(f\"y_pred for locations:\\n {y_loc_pred_ab[:400]}\\n\")\n",
    "print(f\"y_pred for emojis with locations:\\n {y_emo_loc_pred_ab[:400]}\\n\")\n",
    "\n",
    "fold_scores = calc_scores(y_emo_loc_pred_ab, y_emo_test, True)\n",
    "print_scores(fold_scores, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def test_vectorize_func(a, v1, v2):\n",
    "    if a == 1:\n",
    "        return v2\n",
    "    else:\n",
    "        return v1\n",
    "\n",
    "vfunc = np.vectorize(test_vectorize_func)\n",
    "\n",
    "\n",
    "#vec1 = \n",
    "#vec2 = \n",
    "#y_test = \n",
    "\n",
    "average = 'micro'  # for multi_class\n",
    "#average = 'binary'\n",
    "\n",
    "metric1 = []\n",
    "metric2 = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    vec_rand = np.random.randint(2, size=len(vec1))\n",
    "    vec1_new = vfunc(vec_rand, vec1, vec2)\n",
    "    vec2_new = vfunc(vec_rand, vec2, vec1)\n",
    "    \n",
    "    f1_vec1 = f1_score(y_test, vec1_new, average=average) \n",
    "    f1_vec2 = f1_score(y_test, vec2_new, average=average) \n",
    "    \n",
    "    metric1.append(f1_vec1)\n",
    "    metric2.append(f1_vec2)\n",
    "    \n",
    "print(stats.ttest_ind(metric1, metric2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLSTM for emoji location prediction\n",
    "\n",
    "Bidirectional long short-term memory recurrent network implementation using the Keras framework. Emoji location prediction is treated as a binary classification problem. TWEET_NUM determines how much of the tweets are used for training. The model uses an embedding layer and its size is a hyperparameter. Using pretrained Glove embeddings proved to be better. A single bidirectional layer is used which actually consists of two LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "GLOVE_DIR = \"./embeddings/\"\n",
    "GLOVE_FILE_NAME = \"glove.twitter.27B.\"\n",
    "GLOVE_FILE_NAME_EXT = \"d.txt\"\n",
    "\n",
    "MODEL_DIR = \"./models/blstm_models/\"\n",
    "BLSTM_BASE_FILE_NAME = \"blstm_model_\"\n",
    "BLSTM_FILE_NAME_EXT = \".h5\"\n",
    "\n",
    "N_TIMESTEPS = MAX_WORDS_PER_TWEET + 1\n",
    "NUM_EMOJI_TYPES = 20\n",
    "TEST_SPLIT_SIZE = 0.1\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 1\n",
    "MAX_EPOCH = 30\n",
    "\n",
    "TWEET_NUM = len(text_lines)\n",
    "# TWEET_NUM = 100000\n",
    "INPUT_SIZE = int(TWEET_NUM * (1 - TEST_SPLIT_SIZE))\n",
    "NUM_OF_VOCAB = None\n",
    "EMBEDDING_SIZE = 200\n",
    "HIDDEN_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Words are converted to integer ids using a tokenizer. Each unique word has its own unique integer value. Tweets are padded with zeros to a set length. Padding is required by Keras. Since most of the labels are zero (emojis are present after every 15th word), class weights are calculated. Without class weights the model behaves like a majority class classifier which is of no use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_input = text_lines[:TWEET_NUM]\n",
    "y_input = loc_lines[:TWEET_NUM]\n",
    "y_emo_input = emo_lines[:TWEET_NUM]\n",
    "\n",
    "abc = 100000\n",
    "\n",
    "# X_blstm_train, y_blstm_train, y_blstm_emo_train = X_input[:abc], y_input[:abc], y_emo_input[:abc]\n",
    "X_blstm_train, y_blstm_train, y_blstm_emo_train = X_input[:INPUT_SIZE], y_input[:INPUT_SIZE], y_emo_input[:INPUT_SIZE]\n",
    "X_blstm_test, y_blstm_test, y_blstm_emo_test = X_input[INPUT_SIZE:], y_input[INPUT_SIZE:], y_emo_input[INPUT_SIZE:]\n",
    "\n",
    "# X_blstm_train, X_blstm_test, y_blstm_train, y_blstm_test = train_test_split(X_input, y_input, test_size=TEST_SPLIT_SIZE)\n",
    "\n",
    "if NUM_OF_VOCAB is not None:\n",
    "    tokenizer = Tokenizer(num_words=NUM_OF_VOCAB)\n",
    "else:\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "tokenizer.fit_on_texts(X_input)\n",
    "word_index = tokenizer.word_index\n",
    "txt_to_seq = tokenizer.texts_to_sequences(X_blstm_train)\n",
    "# print(f\"encoded:\\n{txt_to_seq[0:5]}\\n\")\n",
    "\n",
    "if NUM_OF_VOCAB is not None:\n",
    "    vocab_size = NUM_OF_VOCAB + 1\n",
    "else:\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "X_blstm = pad_sequences(txt_to_seq, maxlen=N_TIMESTEPS - 1, padding='post')\n",
    "start_padding = np.zeros((X_blstm.shape[0], 1))\n",
    "X_blstm = np.append(start_padding, X_blstm, axis=1).astype(int)\n",
    "print(f\"input shape: {X_blstm.shape}\")\n",
    "print(f\"BLSTM input example:\\n{X_blstm[:5]}\\n\")\n",
    "\n",
    "y_loc = y_blstm_train\n",
    "y_blstm = to_categorical(y_loc, num_classes=2)\n",
    "print(f\"location labels shape: {y_blstm.shape}\\n\")\n",
    "print(f\"BLSTM loc labels:\\n{y_loc[:5]}\\n\")\n",
    "\n",
    "y_emo = y_blstm_emo_train\n",
    "y_emo_blstm = to_categorical(y_emo, num_classes=NUM_EMOJI_TYPES + 1)\n",
    "print(f\"emo labels shape: {y_emo_blstm.shape}\\n\")\n",
    "print(f\"BLSTM emo labels:\\n{y_emo[:5]}\\n\")\n",
    "\n",
    "def calc_sample_weights(y):\n",
    "#     print(np.unique(y))\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y.flatten())\n",
    "    class_weight_dict = dict(enumerate(weights))\n",
    "#     print(f\"class weight dict:\\n{class_weight_dict}\\n\")\n",
    "    vfunc = np.vectorize(lambda x: class_weight_dict[x])\n",
    "    return vfunc(y)\n",
    "\n",
    "sample_weights_loc = calc_sample_weights(y_loc)\n",
    "print(f\"sample_weights_loc shape: {sample_weights_loc.shape}\")\n",
    "print(f\"sample_weights_loc examples:\\n{sample_weights_loc[:5]}\\n\")\n",
    "\n",
    "sample_weights_emo = calc_sample_weights(y_emo)\n",
    "print(f\"sample_weights_emo shape: {sample_weights_emo.shape}\")\n",
    "print(f\"sample_weights_emo examples:\\n{sample_weights_emo[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_embedding_matrix(glove_size):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, GLOVE_FILE_NAME + str(glove_size) + GLOVE_FILE_NAME_EXT), encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, glove_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size - 1:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i + 1] = embedding_vector\n",
    "\n",
    "    print(f\"embedding matrix shape {embedding_matrix.shape}\")\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "glove_25 = None\n",
    "glove_50 = None\n",
    "glove_100 = None\n",
    "glove_200 = None\n",
    "\n",
    "def get_embedding_matrix(glove_size):\n",
    "    global glove_25\n",
    "    global glove_50\n",
    "    global glove_100\n",
    "    global glove_200\n",
    "    \n",
    "    if glove_size == 25:\n",
    "        if glove_25 is None:\n",
    "            glove_25 = load_embedding_matrix(25)\n",
    "        return glove_25\n",
    "    \n",
    "    if glove_size == 50:\n",
    "        if glove_50 is None:\n",
    "            glove_50 = load_embedding_matrix(50)\n",
    "        return glove_50\n",
    "    \n",
    "    if glove_size == 100:\n",
    "        if glove_100 is None:\n",
    "            glove_100 = load_embedding_matrix(100)\n",
    "        return glove_100\n",
    "    \n",
    "    if glove_size == 200:\n",
    "        if glove_200 is None:\n",
    "            glove_200 = load_embedding_matrix(200)\n",
    "        return glove_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train model\n",
    " \n",
    " The following code is used to create and train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def get_param_str(embedding_size, hidden_size, n_classes, input_size):\n",
    "    param_str = \"CLA-\" + str(n_classes) + \"_INP-\" + str(input_size) + \"_EMB-\" + str(embedding_size) + \"_HID-\" + str(hidden_size)\n",
    "    return param_str\n",
    "\n",
    "def get_blstm_file_name(embedding_size, hidden_size, n_classes, input_size):\n",
    "    param_str = get_param_str(embedding_size, hidden_size, n_classes, input_size)\n",
    "    return MODEL_DIR + BLSTM_BASE_FILE_NAME + param_str + BLSTM_FILE_NAME_EXT\n",
    "\n",
    "def save_blstm(blstm, embedding_size, hidden_size, n_classes=2, input_size=TWEET_NUM):\n",
    "    blstm.save(get_blstm_file_name(embedding_size, hidden_size, n_classes, input_size))\n",
    "\n",
    "def load_blstm(embedding_size, hidden_size, n_classes=2, input_size=TWEET_NUM):\n",
    "    return load_model(get_blstm_file_name(embedding_size, hidden_size, n_classes, input_size))\n",
    "\n",
    "def get_callbacks():\n",
    "    callbacks = []\n",
    "    \n",
    "    callbacks.append(EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=EARLY_STOPPING_PATIENCE,\n",
    "                              verbose=0, mode='auto'))\n",
    "    checkpoint_path = MODEL_DIR + BLSTM_BASE_FILE_NAME + \"check\" + BLSTM_FILE_NAME_EXT\n",
    "    callbacks.append(ModelCheckpoint(filepath=checkpoint_path, save_best_only=True))\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def get_bi_lstm_model(embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE, n_classes=2, use_glove=True,\n",
    "                      n_timesteps=N_TIMESTEPS, mode=\"concat\"):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if use_glove:\n",
    "        embedding_matrix = get_embedding_matrix(embedding_size)\n",
    "        model.add(Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=N_TIMESTEPS,\n",
    "                            trainable=True))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size_size,\n",
    "                            embedding_size,\n",
    "                            input_length=N_TIMESTEPS))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode=mode))\n",
    "    model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "#     binary_crossentropy\n",
    "#     categorical_crossentropy\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"], sample_weight_mode=\"temporal\")\n",
    "    return model\n",
    "\n",
    "def get_predictions(blstm, texts):\n",
    "    text_to_integer_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    blstm_input = pad_sequences(text_to_integer_sequences, maxlen=N_TIMESTEPS, padding='post')\n",
    "    ypred = blstm.predict_classes(blstm_input)\n",
    "    return ypred\n",
    "\n",
    "def evaluate_blstm(blstm, texts=X_blstm_test, labels=y_blstm_test):\n",
    "    predictions = get_predictions(blstm, texts)\n",
    "    print(predictions.shape)\n",
    "    print(labels.shape)\n",
    "    print(predictions[:10])\n",
    "    print(labels[:10])\n",
    "    print_scores(calc_scores(predictions.flatten(), labels.flatten(), np.max(labels)>1))\n",
    "\n",
    "def do_blstm_test(embedding_size, hidden_size, is_emo=False, use_glove=True, input_size=TWEET_NUM):\n",
    "    global blstm\n",
    "    \n",
    "    if is_emo:\n",
    "        y_train = y_emo_blstm\n",
    "        y_test = y_blstm_emo_test\n",
    "        sample_weights = sample_weights_emo\n",
    "    else:\n",
    "        y_train = y_blstm\n",
    "        y_test = y_blstm_test\n",
    "        sample_weights = sample_weights_loc\n",
    "        \n",
    "    n_classes = y_train.shape[2]\n",
    "    blstm = get_bi_lstm_model(embedding_size, hidden_size, n_classes, use_glove=use_glove)\n",
    "    print(f\"\\nTraining with embedding {embedding_size} hidden {hidden_size} classes {n_classes} input {input_size} glove {use_glove}\")    \n",
    "    blstm.fit(X_blstm, y_train, epochs=MAX_EPOCH,\n",
    "              validation_split=VALIDATION_SPLIT, verbose=2,\n",
    "              sample_weight=sample_weights, callbacks=get_callbacks())\n",
    "    save_blstm(blstm, embedding_size, hidden_size, n_classes, input_size)\n",
    "    try:\n",
    "        evaluate_blstm(blstm, labels=y_test)\n",
    "    except Exception:\n",
    "        print(\"couldn't evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_blstm_test(200, 500, is_emo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_sizes = [200, 100]\n",
    "hidden_sizes = [200, 500, 1000]\n",
    "\n",
    "blstm = None\n",
    "for embedding_size in embedding_sizes:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        do_blstm_test(embedding_size, hidden_size, is_emo=False)\n",
    "        \n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "\n",
    "do_blstm_test(200, 501, is_emo=False)\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 1\n",
    "\n",
    "do_blstm_test(200, 500, is_emo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do_blstm_test(200, 500, is_emo=True)\n",
    "\n",
    "blstm = load_blstm(200, 1000, 2)\n",
    "blstm_emo = load_blstm(200, 500, 21)\n",
    "imtired = 20\n",
    "inputs = X_blstm_test[:imtired]\n",
    "for i in range(imtired):\n",
    "    print(inputs[i])\n",
    "labels = y_blstm_test[:imtired]\n",
    "labels_emo = y_blstm_emo_test[:imtired]\n",
    "\n",
    "evaluate_blstm(blstm, inputs, labels)\n",
    "evaluate_blstm(blstm_emo, inputs, labels_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_blstm_test(200, 500, is_emo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_blstm(blstm, labels=y_blstm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_blstm(blstm, EMBEDDING_SIZE, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for embedding_size in embedding_sizes:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        blstm = load_blstm(embedding_size, hidden_size, 2, 100000)\n",
    "        evaluate_blstm(blstm, labels=y_blstm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for embedding_size in embedding_sizes:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        blstm = load_blstm(embedding_size, hidden_size)\n",
    "        print(f\"\\nEmbedding {embedding_size} hidden {hidden_size}\")\n",
    "        evaluate_blstm(blstm, X_blstm_test, y_blstm_test)\n",
    "        print(f\"test:\")\n",
    "        evaluate_blstm(blstm, X_blstm_test, y_blstm_test)\n",
    "        print(f\"\\ntrain:\")\n",
    "        evaluate_blstm(blstm, X_blstm_train, y_blstm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model\n",
    "\n",
    "The following block is used to load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blstm = load_model(BLSTM_MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_TWEETS_TO_TEST = 10\n",
    "\n",
    "print(f\"example of labels (emoji locations):\\n{y_input[:NUM_OF_TWEETS_TO_TEST]}\\n\")\n",
    "print(f\"model predictions:\\n{get_predictions(blstm, X_input[:NUM_OF_TWEETS_TO_TEST])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Save predictions\n",
    "\n",
    "Use the code below to save/load a model's predictions. Training and testing the model requires a powerful Nvidia GPU and is time consuming. This also enables the evaluation of results without having to work with the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blstm_predictions_flag = False\n",
    "\n",
    "if blstm_predictions_flag:\n",
    "    BLSTM_PREDICTIONS_FILE_NAME = \"blstm_predictions_\" + PARAMETER_STR + \".pkl\"\n",
    "    predictions = get_predictions(text_lines)\n",
    "\n",
    "    result_dict = {}\n",
    "    result_dict[\"num_of_classes\"] = N_CLASSES\n",
    "    result_dict[\"num_of_input_tweets\"] = TWEET_NUM\n",
    "    result_dict[\"embedding_size\"] = EMBEDDING_SIZE\n",
    "    result_dict[\"hidden_size\"] = HIDDEN_SIZE\n",
    "    result_dict[\"input_X\"] = X_blstm\n",
    "    result_dict[\"input_y\"] = y_blstm\n",
    "    result_dict[\"ypred\"] = predictions\n",
    "\n",
    "    joblib.dump(result_dict, BLSTM_PREDICTIONS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if blstm_predictions_flag:\n",
    "    result_dict = joblib.load(BLSTM_PREDICTIONS_FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
